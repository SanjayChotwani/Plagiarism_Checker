{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17H9jLTnTBhm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    # Perform stemming on the words\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the stemmed tokens back into a single string\n",
        "    preprocessed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    preprocessed_text1 = preprocess_text(text1)\n",
        "    preprocessed_text2 = preprocess_text(text2)\n",
        "\n",
        "    # Create a set of unique words from the preprocessed texts\n",
        "    unique_words = set(preprocessed_text1.split() + preprocessed_text2.split())\n",
        "\n",
        "    # Create frequency vectors for each text\n",
        "    vector1 = [preprocessed_text1.split().count(word) for word in unique_words]\n",
        "    vector2 = [preprocessed_text2.split().count(word) for word in unique_words]\n",
        "\n",
        "    # Calculate the cosine similarity between the two vectors\n",
        "    dot_product = sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n",
        "    magnitude1 = sum(v1 ** 2 for v1 in vector1) ** 0.5\n",
        "    magnitude2 = sum(v2 ** 2 for v2 in vector2) ** 0.5\n",
        "    similarity = dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "    return similarity\n"
      ],
      "metadata": {
        "id": "WnQ032-UTfkB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67ECTcsLTps1",
        "outputId": "e926f0ea-5451-4921-b2d3-3a202f36e7f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document1 = \"\"\"\n",
        "It is a platform which connects all peoples together.\n",
        "We can made our friend circle on the social media.\n",
        "We can connect to our old friends too.\n",
        "We can make new friends on social media from foreign countries also.\n",
        "At a time we can stay connected with more peoples.\n",
        "\"\"\"\n",
        "document2 = \"\"\"\n",
        "It is a platform which connects all people.\n",
        "We can make new friends on social media from foreign countries also.\n",
        "\"\"\"\n",
        "\n",
        "similarity_score = calculate_similarity(document1, document2)\n",
        "print(f\"Similarity score: {similarity_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cQyb7B9Tk4Y",
        "outputId": "37d6e27e-958d-4c9c-864c-c9bf6acede81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score: 0.8374357893586237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpytv3F_VgyF",
        "outputId": "7d5339dd-7ed3-4088-a4ef-2b39cc2eae5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def check_plagiarism(original_text, suspicious_text):\n",
        "    # Calculate the similarity ratio between the texts\n",
        "    similarity_ratio = fuzz.token_set_ratio(original_text, suspicious_text)\n",
        "\n",
        "    # Print the similarity ratio\n",
        "    print(f\"Similarity ratio: {similarity_ratio}%\")\n",
        "\n",
        "    # Determine if the content is plagiarized based on a threshold\n",
        "    threshold = 70  # Adjust as needed\n",
        "    if similarity_ratio >= threshold:\n",
        "        print(\"The content is likely plagiarized.\")\n",
        "    else:\n",
        "        print(\"The content is original.\")\n",
        "\n",
        "# Usage example\n",
        "original_text = \"\"\"\n",
        "Health is the biggest wealth for a human being in his/her entire lifetime.\n",
        "One can survive without excess money but can't survive without good health.\n",
        "Health is something that we can't buy with money but we can take care of it and we can cure it when needed with the help of the money.\n",
        "\"\"\"\n",
        "\n",
        "suspicious_text = \"\"\"\n",
        "It is said in this proverb that health of a man is as much important as the wealth to live a healthy, peaceful and prosperous life.\n",
        "One can survive without excess money but can't survive without good health.\n",
        "\"\"\"\n",
        "\n",
        "check_plagiarism(original_text, suspicious_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggEYxMRVUT2w",
        "outputId": "f3bee965-1e0c-49dc-a0da-153c2a258e5c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity ratio: 68%\n",
            "The content is original.\n"
          ]
        }
      ]
    }
  ]
}